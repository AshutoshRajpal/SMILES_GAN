{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb3d316-3162-4420-a2ed-ce40b104ef89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized and padded SMILES sequences:\n",
      "[[1 1 3 2 4 5 4 1 6 2 1 1 2 1 1 2 1 6 1 3 2 4 5 4 0 0 0 0 0]\n",
      " [1 1 3 2 4 5 7 1 6 2 1 1 2 1 3 1 2 1 6 5 4 0 0 0 0 0 0 0 0]\n",
      " [1 1 3 1 5 1 1 6 2 1 1 2 1 3 1 2 1 6 5 1 3 1 5 1 3 2 4 5 4]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load your cleaned SMILES data\n",
    "df = pd.read_csv(\"cleaned_smiles_data.csv\")\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer(char_level=True)  # Tokenize at the character level for SMILES\n",
    "tokenizer.fit_on_texts(df['Canonical SMILES'])\n",
    "\n",
    "# Convert SMILES strings to sequences\n",
    "sequences = tokenizer.texts_to_sequences(df['Canonical SMILES'])\n",
    "\n",
    "# Pad sequences to the same length\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding=\"post\")\n",
    "\n",
    "print(\"Tokenized and padded SMILES sequences:\")\n",
    "print(padded_sequences)\n",
    "\n",
    "# Save tokenizer for later use\n",
    "import pickle\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c7f8338-206e-4800-bc72-090fdb0e3cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step\n",
      "D Loss: [0.6973576 0.609375 ], G Loss: 0.6849991083145142\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x16608c5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x166709d00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "D Loss: [0.6962564 0.75     ], G Loss: 0.672376811504364\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "D Loss: [0.7039097 0.75     ], G Loss: 0.6547790169715881\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "D Loss: [0.7332922 0.75     ], G Loss: 0.6290233135223389\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "D Loss: [0.8195772 0.75     ], G Loss: 0.5919936895370483\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "D Loss: [1.0046253 0.75     ], G Loss: 0.5493327975273132\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "D Loss: [1.0859011 0.75     ], G Loss: 0.5483332872390747\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "D Loss: [1.1299332 0.75     ], G Loss: 0.5571203231811523\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "D Loss: [1.4306264 0.75     ], G Loss: 0.5253899693489075\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "D Loss: [1.7946012 0.75     ], G Loss: 0.4793871343135834\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 10  # Example vocabulary size\n",
    "embedding_dim = 8\n",
    "max_length = 29  # Maximum sequence length\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Generator\n",
    "def build_generator(vocab_size, max_length):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Input(shape=(max_length,)),\n",
    "        layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n",
    "        layers.LSTM(256, return_sequences=True),\n",
    "        layers.LSTM(256, return_sequences=True),\n",
    "        layers.TimeDistributed(layers.Dense(embedding_dim)),  # Produces embedding vectors\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Discriminator\n",
    "def build_discriminator(max_length, embedding_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Input(shape=(max_length, embedding_dim)),  # Accepts embedded sequences\n",
    "        layers.LSTM(256, return_sequences=True),\n",
    "        layers.LSTM(256),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),  # Binary classification\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# Build models\n",
    "generator = build_generator(vocab_size, max_length)\n",
    "discriminator = build_discriminator(max_length, embedding_dim)\n",
    "\n",
    "# GAN\n",
    "discriminator.trainable = False\n",
    "gan_input = tf.keras.Input(shape=(max_length,))\n",
    "generator_output = generator(gan_input)  # Generator produces embeddings\n",
    "gan_output = discriminator(generator_output)  # Pass generated embeddings to discriminator\n",
    "gan = tf.keras.Model(gan_input, gan_output)\n",
    "gan.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "# Separate embedding layer for real sequences\n",
    "embedding_layer = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length)\n",
    "\n",
    "# Sample Data\n",
    "real_sequences = np.random.randint(1, vocab_size, size=(batch_size, max_length))  # Real tokenized sequences\n",
    "fake_sequences = np.random.randint(1, vocab_size, size=(batch_size, max_length))  # Random noise\n",
    "\n",
    "# Labels\n",
    "real_labels = np.ones((batch_size, 1))  # Real data label\n",
    "fake_labels = np.zeros((batch_size, 1))  # Fake data label\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    # Recompile discriminator to reset internal states\n",
    "    discriminator.trainable = True\n",
    "    discriminator.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    # Embed real sequences using the standalone embedding layer\n",
    "    real_data_embedded = embedding_layer(real_sequences)  # Embed real sequences\n",
    "    \n",
    "    # Train Discriminator\n",
    "    fake_data = generator.predict(fake_sequences)  # Generate fake data embeddings\n",
    "    d_loss_real = discriminator.train_on_batch(real_data_embedded, real_labels)\n",
    "    d_loss_fake = discriminator.train_on_batch(fake_data, fake_labels)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "    \n",
    "    # Train Generator\n",
    "    discriminator.trainable = False\n",
    "    noise = np.random.randint(1, vocab_size, size=(batch_size, max_length))\n",
    "    g_loss = gan.train_on_batch(noise, real_labels)  # Train generator to fool discriminator\n",
    "    \n",
    "    print(f\"D Loss: {d_loss}, G Loss: {g_loss}\")              "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
